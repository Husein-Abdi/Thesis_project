{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80df0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed4732c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['datetime', 'guri_num', 'degmada', 'total_KW'], dtype='object')\n",
      "0:\tlearn: 39.9781308\ttotal: 41.9ms\tremaining: 41.9s\n",
      "100:\tlearn: 23.2117998\ttotal: 4.96s\tremaining: 44.1s\n",
      "200:\tlearn: 20.7557630\ttotal: 9.63s\tremaining: 38.3s\n",
      "300:\tlearn: 19.2349156\ttotal: 14.4s\tremaining: 33.4s\n",
      "400:\tlearn: 18.1441464\ttotal: 19.3s\tremaining: 28.8s\n",
      "500:\tlearn: 17.4554544\ttotal: 23.9s\tremaining: 23.8s\n",
      "600:\tlearn: 16.9524639\ttotal: 28.5s\tremaining: 18.9s\n",
      "700:\tlearn: 16.5224416\ttotal: 33.2s\tremaining: 14.2s\n",
      "800:\tlearn: 16.1596499\ttotal: 37.8s\tremaining: 9.38s\n",
      "900:\tlearn: 15.8641785\ttotal: 42.3s\tremaining: 4.65s\n",
      "999:\tlearn: 15.5842698\ttotal: 46.9s\tremaining: 0us\n",
      "Root Mean Squared Error: 15.720821600311991\n",
      "R² Score: 0.8624713036473084\n",
      "Mean Absolute Error: 12.110870270426457\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('modified_electricity_consumption_data5.csv')\n",
    "\n",
    "# Check the columns to verify if 'datetime' is present\n",
    "print(\"Columns in the dataset:\", df.columns)\n",
    "\n",
    "# Convert 'datetime' column to datetime data type if present\n",
    "if 'datetime' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "else:\n",
    "    print(\"Column 'datetime' not found in DataFrame.\")\n",
    "\n",
    "# Aggregate daily data to monthly data\n",
    "df['year_month'] = df['datetime'].dt.to_period('M')\n",
    "monthly_df = df.groupby(['year_month', 'guri_num', 'degmada'])['total_KW'].sum().reset_index()\n",
    "\n",
    "# Extract month and year from the year_month column\n",
    "monthly_df['year'] = monthly_df['year_month'].dt.year\n",
    "monthly_df['month'] = monthly_df['year_month'].dt.month\n",
    "\n",
    "# Select the features and target variable\n",
    "features = ['guri_num', 'degmada', 'month', 'year']\n",
    "target = 'total_KW'\n",
    "\n",
    "X = monthly_df[features]\n",
    "y = monthly_df[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Specify the categorical features\n",
    "categorical_features = ['degmada']\n",
    "\n",
    "# Create the Pool object for CatBoost\n",
    "train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features)\n",
    "test_pool = Pool(data=X_test, label=y_test, cat_features=categorical_features)\n",
    "\n",
    "# Train a CatBoost Regressor\n",
    "model = CatBoostRegressor(iterations=1000, depth=6, learning_rate=0.1, loss_function='RMSE', random_seed=42)\n",
    "model.fit(train_pool, verbose=100)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(test_pool)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da69fb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year_month  guri_num degmada  year  month  predicted_total_KW\n",
      "0  2023-01-31     10349  Shibis  2023      1           91.726891\n",
      "1  2023-02-28     10349  Shibis  2023      2          128.784529\n",
      "2  2023-03-31     10349  Shibis  2023      3          151.429874\n",
      "3  2023-04-30     10349  Shibis  2023      4          140.947065\n",
      "4  2023-05-31     10349  Shibis  2023      5          139.446784\n",
      "5  2023-06-30     10349  Shibis  2023      6           87.387613\n",
      "6  2023-07-31     10349  Shibis  2023      7           79.387856\n",
      "7  2023-08-31     10349  Shibis  2023      8           70.044841\n",
      "8  2023-09-30     10349  Shibis  2023      9           61.711399\n",
      "9  2023-10-31     10349  Shibis  2023     10           63.362899\n",
      "10 2023-11-30     10349  Shibis  2023     11           67.248467\n",
      "11 2023-12-31     10349  Shibis  2023     12           79.290036\n",
      "12 2024-01-31     10349  Shibis  2024      1           91.726891\n",
      "13 2024-02-29     10349  Shibis  2024      2          128.784529\n",
      "14 2024-03-31     10349  Shibis  2024      3          151.429874\n",
      "15 2024-04-30     10349  Shibis  2024      4          140.947065\n",
      "16 2024-05-31     10349  Shibis  2024      5          139.446784\n",
      "17 2024-06-30     10349  Shibis  2024      6           87.387613\n",
      "18 2024-07-31     10349  Shibis  2024      7           79.387856\n",
      "19 2024-08-31     10349  Shibis  2024      8           70.044841\n",
      "20 2024-09-30     10349  Shibis  2024      9           61.711399\n",
      "21 2024-10-31     10349  Shibis  2024     10           63.362899\n",
      "22 2024-11-30     10349  Shibis  2024     11           67.248467\n",
      "23 2024-12-31     10349  Shibis  2024     12           79.290036\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare for future predictions\n",
    "# Extract unique house numbers (guri_num) from your dataset\n",
    "valid_guri_nums = monthly_df['guri_num'].unique()\n",
    "\n",
    "# Generate future months for prediction\n",
    "future_dates = pd.date_range(start='2023-01-01', end='2025-12-31', freq='M')\n",
    "\n",
    "# Create a DataFrame for future predictions\n",
    "future_df = pd.DataFrame({\n",
    "    'year_month': np.tile(future_dates, len(valid_guri_nums)),\n",
    "    'guri_num': np.repeat(valid_guri_nums, len(future_dates))\n",
    "})\n",
    "\n",
    "# Add district column by merging with unique guri_num to district mapping\n",
    "guri_district_mapping = monthly_df[['guri_num', 'degmada']].drop_duplicates()\n",
    "future_df = future_df.merge(guri_district_mapping, on='guri_num', how='left')\n",
    "\n",
    "# Extract month and year for future dates\n",
    "future_df['year'] = future_df['year_month'].dt.year\n",
    "future_df['month'] = future_df['year_month'].dt.month\n",
    "\n",
    "# Select the features for future prediction\n",
    "X_future = future_df[['guri_num', 'degmada', 'month', 'year']]\n",
    "\n",
    "# Create the Pool object for future data\n",
    "future_pool = Pool(data=X_future, cat_features=categorical_features)\n",
    "\n",
    "# Predict future electricity consumption using the trained CatBoost model\n",
    "future_df['predicted_total_KW'] = model.predict(future_pool)\n",
    "\n",
    "# Display the future predictions\n",
    "print(future_df.head(24))\n",
    "\n",
    "# Save the future predictions to CSV\n",
    "# future_df.to_csv('future_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caeb3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xusee\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 991ms/step - loss: 0.1730 - val_loss: 0.1728\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.1647 - val_loss: 0.1655\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.1606 - val_loss: 0.1554\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.1430 - val_loss: 0.1426\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.1239 - val_loss: 0.1281\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1230 - val_loss: 0.1131\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.1073 - val_loss: 0.0990\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0970 - val_loss: 0.0863\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.0820 - val_loss: 0.0750\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0770 - val_loss: 0.0652\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0660 - val_loss: 0.0569\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0603 - val_loss: 0.0500\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0513 - val_loss: 0.0443\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0476 - val_loss: 0.0399\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0423 - val_loss: 0.0364\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0393 - val_loss: 0.0334\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0380 - val_loss: 0.0309\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.0311 - val_loss: 0.0282\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0349 - val_loss: 0.0264\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0291 - val_loss: 0.0267\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0293 - val_loss: 0.0216\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0245 - val_loss: 0.0204\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0204 - val_loss: 0.0230\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0227 - val_loss: 0.0154\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0161 - val_loss: 0.0147\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0151 - val_loss: 0.0196\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0139 - val_loss: 0.0105\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0130 - val_loss: 0.0186\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0135 - val_loss: 0.0098\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0125 - val_loss: 0.0097\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0113 - val_loss: 0.0215\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0147 - val_loss: 0.0135\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0105 - val_loss: 0.0093\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0121 - val_loss: 0.0120\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0118 - val_loss: 0.0080\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0125 - val_loss: 0.0095\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0091 - val_loss: 0.0180\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0128 - val_loss: 0.0138\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0100 - val_loss: 0.0082\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.0098 - val_loss: 0.0081\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0097 - val_loss: 0.0134\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0111 - val_loss: 0.0086\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0092 - val_loss: 0.0080\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0103 - val_loss: 0.0097\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0088 - val_loss: 0.0108\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0091 - val_loss: 0.0079\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0087 - val_loss: 0.0072\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0111 - val_loss: 0.0092\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677ms/step\n",
      "Root Mean Squared Error: 17.89833559748302\n",
      "R² Score: 0.7378691932655433\n",
      "Mean Absolute Error: 13.621819708419984\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001AF1B62B880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step \n",
      "           BOO13096   BOO14883   BOO16425   BOO21552    BOO24958   BOO25971  \\\n",
      "2022-01  120.686211  85.631027  59.418957  79.329590  132.092148  66.229462   \n",
      "2022-02   94.743477  65.351471  44.633678  58.082558  105.970062  56.541008   \n",
      "2022-03   95.012154  62.812672  44.372078  56.743256  105.431168  56.377117   \n",
      "2022-04   97.244232  63.049942  44.793835  58.556602  107.129845  56.791367   \n",
      "2022-05  100.033386  64.974960  45.489082  61.704323  108.850449  57.601845   \n",
      "2022-06   95.451950  63.046989  42.721516  60.903755  102.713905  54.663330   \n",
      "2022-07   90.616829  60.122913  39.111603  60.209080   95.885681  51.958797   \n",
      "2022-08   80.693085  53.904797  33.408154  56.951576   84.052330  46.708103   \n",
      "2022-09   68.687141  47.427540  28.423187  51.504040   70.833412  40.985981   \n",
      "2022-10   56.123119  40.924061  24.981037  44.628078   57.771664  35.538578   \n",
      "2022-11   44.352085  34.960121  22.867279  37.519806   45.736431  30.508930   \n",
      "2022-12   33.736816  29.849489  20.636766  31.252817   35.588192  26.167912   \n",
      "\n",
      "          BOO26776   BOO27637    BOO34137    BOO38048  ...   YAA39553  \\\n",
      "2022-01  84.188454  83.890556  128.193329  116.358978  ...  81.259453   \n",
      "2022-02  63.587894  58.935238   91.954208   90.271011  ...  63.984825   \n",
      "2022-03  62.928303  56.103378   87.397957   89.304283  ...  61.514465   \n",
      "2022-04  63.978168  57.172749   87.348038   89.929466  ...  61.679081   \n",
      "2022-05  65.612625  60.860142   89.191429   90.914520  ...  63.142696   \n",
      "2022-06  63.266079  61.443844   83.396904   84.384308  ...  60.416416   \n",
      "2022-07  60.585785  61.261726   76.488693   76.684441  ...  56.889999   \n",
      "2022-08  54.414551  57.370792   64.786240   64.482040  ...  49.901802   \n",
      "2022-09  46.614120  51.796684   54.033718   53.069176  ...  42.719601   \n",
      "2022-10  38.840691  45.454315   45.349579   43.678150  ...  36.214848   \n",
      "2022-11  31.483822  39.111916   38.549591   36.385620  ...  30.513611   \n",
      "2022-12  25.338928  32.981075   33.403236   30.630341  ...  25.997044   \n",
      "\n",
      "           YAA44919    YAA62105   YAA63403   YAA70435   YAA78103    YAA82926  \\\n",
      "2022-01  133.293671  104.220734  63.758144  86.888268  85.115448  123.115776   \n",
      "2022-02   92.949623   85.154861  54.315800  69.060829  65.729141  102.109634   \n",
      "2022-03   89.331612   84.913750  55.107216  66.101524  63.641083  101.105652   \n",
      "2022-04   90.070580   85.203316  55.211010  65.696533  63.806400  101.437111   \n",
      "2022-05   92.720497   85.269234  55.038193  66.660950  64.042198  102.196762   \n",
      "2022-06   88.711304   78.160210  50.650383  62.618465  58.874943   95.643097   \n",
      "2022-07   83.585091   70.325806  43.740887  57.369785  53.281174   87.664482   \n",
      "2022-08   73.785202   58.796917  34.190388  48.984783  45.163292   75.677673   \n",
      "2022-09   63.249302   48.519642  27.349520  41.900490  37.924385   64.728333   \n",
      "2022-10   53.547291   40.198143  23.651558  36.619545  31.884012   55.017902   \n",
      "2022-11   44.538044   33.725895  22.139240  33.272003  27.609999   46.409676   \n",
      "2022-12   36.220360   29.243963  20.889088  31.256489  24.606819   38.851986   \n",
      "\n",
      "          YAA96787   YAA97424    YAA97684  \n",
      "2022-01  80.124786  95.139969  110.635857  \n",
      "2022-02  57.531342  78.074059   85.630547  \n",
      "2022-03  57.059055  76.898666   84.520988  \n",
      "2022-04  59.264389  77.271271   85.581039  \n",
      "2022-05  63.153439  78.544708   87.712318  \n",
      "2022-06  63.999271  75.533966   83.460258  \n",
      "2022-07  64.686890  71.710434   78.638580  \n",
      "2022-08  61.711952  63.983265   69.275642  \n",
      "2022-09  55.455555  55.353088   58.942738  \n",
      "2022-10  47.497543  46.716927   48.941048  \n",
      "2022-11  38.893387  38.788712   40.246796  \n",
      "2022-12  30.465832  31.303543   32.168381  \n",
      "\n",
      "[12 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('modified_electricity_consumption_data.csv')\n",
    "\n",
    "# Convert 'datetime' to datetime data type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Aggregate daily data to monthly data\n",
    "df['year_month'] = df['datetime'].dt.to_period('M')\n",
    "monthly_df = df.groupby(['year_month', 'guri_num'])['total_KW'].sum().reset_index()\n",
    "\n",
    "# Extract month and year from the year_month column\n",
    "monthly_df['year'] = monthly_df['year_month'].dt.year\n",
    "monthly_df['month'] = monthly_df['year_month'].dt.month\n",
    "\n",
    "# Select the features and target variable\n",
    "features = ['guri_num', 'month', 'year']\n",
    "target = 'total_KW'\n",
    "\n",
    "# Normalize the target variable\n",
    "scaler = MinMaxScaler()\n",
    "monthly_df[target] = scaler.fit_transform(monthly_df[[target]])\n",
    "\n",
    "# Pivot data to have time series for each house number (guri_num)\n",
    "pivot_df = monthly_df.pivot(index='year_month', columns='guri_num', values='total_KW').fillna(0)\n",
    "\n",
    "# Prepare the dataset for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "SEQ_LENGTH = 12  # Using 12 months (1 year) as the sequence length\n",
    "data = pivot_df.values\n",
    "\n",
    "X, y = create_sequences(data, SEQ_LENGTH)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(SEQ_LENGTH, X_train.shape[2])),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25),\n",
    "    Dense(X_train.shape[2])\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "y_test_inverse = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred.reshape(-1, 1)).reshape(y_pred.shape)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6235fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "           BOO13096   BOO14883   BOO16425   BOO21552    BOO24958   BOO25971  \\\n",
      "2025-01  120.686211  85.631027  59.418957  79.329590  132.092148  66.229462   \n",
      "2025-02   94.743477  65.351471  44.633678  58.082558  105.970062  56.541008   \n",
      "2025-03   95.012154  62.812672  44.372078  56.743256  105.431168  56.377117   \n",
      "2025-04   97.244232  63.049942  44.793835  58.556602  107.129845  56.791367   \n",
      "2025-05  100.033386  64.974960  45.489082  61.704323  108.850449  57.601845   \n",
      "2025-06   95.451950  63.046989  42.721516  60.903755  102.713905  54.663330   \n",
      "2025-07   90.616829  60.122913  39.111603  60.209080   95.885681  51.958797   \n",
      "2025-08   80.693085  53.904797  33.408154  56.951576   84.052330  46.708103   \n",
      "2025-09   68.687141  47.427540  28.423187  51.504040   70.833412  40.985981   \n",
      "2025-10   56.123119  40.924061  24.981037  44.628078   57.771664  35.538578   \n",
      "2025-11   44.352085  34.960121  22.867279  37.519806   45.736431  30.508930   \n",
      "2025-12   33.736816  29.849489  20.636766  31.252817   35.588192  26.167912   \n",
      "\n",
      "          BOO26776   BOO27637    BOO34137    BOO38048  ...   YAA39553  \\\n",
      "2025-01  84.188454  83.890556  128.193329  116.358978  ...  81.259453   \n",
      "2025-02  63.587894  58.935238   91.954208   90.271011  ...  63.984825   \n",
      "2025-03  62.928303  56.103378   87.397957   89.304283  ...  61.514465   \n",
      "2025-04  63.978168  57.172749   87.348038   89.929466  ...  61.679081   \n",
      "2025-05  65.612625  60.860142   89.191429   90.914520  ...  63.142696   \n",
      "2025-06  63.266079  61.443844   83.396904   84.384308  ...  60.416416   \n",
      "2025-07  60.585785  61.261726   76.488693   76.684441  ...  56.889999   \n",
      "2025-08  54.414551  57.370792   64.786240   64.482040  ...  49.901802   \n",
      "2025-09  46.614120  51.796684   54.033718   53.069176  ...  42.719601   \n",
      "2025-10  38.840691  45.454315   45.349579   43.678150  ...  36.214848   \n",
      "2025-11  31.483822  39.111916   38.549591   36.385620  ...  30.513611   \n",
      "2025-12  25.338928  32.981075   33.403236   30.630341  ...  25.997044   \n",
      "\n",
      "           YAA44919    YAA62105   YAA63403   YAA70435   YAA78103    YAA82926  \\\n",
      "2025-01  133.293671  104.220734  63.758144  86.888268  85.115448  123.115776   \n",
      "2025-02   92.949623   85.154861  54.315800  69.060829  65.729141  102.109634   \n",
      "2025-03   89.331612   84.913750  55.107216  66.101524  63.641083  101.105652   \n",
      "2025-04   90.070580   85.203316  55.211010  65.696533  63.806400  101.437111   \n",
      "2025-05   92.720497   85.269234  55.038193  66.660950  64.042198  102.196762   \n",
      "2025-06   88.711304   78.160210  50.650383  62.618465  58.874943   95.643097   \n",
      "2025-07   83.585091   70.325806  43.740887  57.369785  53.281174   87.664482   \n",
      "2025-08   73.785202   58.796917  34.190388  48.984783  45.163292   75.677673   \n",
      "2025-09   63.249302   48.519642  27.349520  41.900490  37.924385   64.728333   \n",
      "2025-10   53.547291   40.198143  23.651558  36.619545  31.884012   55.017902   \n",
      "2025-11   44.538044   33.725895  22.139240  33.272003  27.609999   46.409676   \n",
      "2025-12   36.220360   29.243963  20.889088  31.256489  24.606819   38.851986   \n",
      "\n",
      "          YAA96787   YAA97424    YAA97684  \n",
      "2025-01  80.124786  95.139969  110.635857  \n",
      "2025-02  57.531342  78.074059   85.630547  \n",
      "2025-03  57.059055  76.898666   84.520988  \n",
      "2025-04  59.264389  77.271271   85.581039  \n",
      "2025-05  63.153439  78.544708   87.712318  \n",
      "2025-06  63.999271  75.533966   83.460258  \n",
      "2025-07  64.686890  71.710434   78.638580  \n",
      "2025-08  61.711952  63.983265   69.275642  \n",
      "2025-09  55.455555  55.353088   58.942738  \n",
      "2025-10  47.497543  46.716927   48.941048  \n",
      "2025-11  38.893387  38.788712   40.246796  \n",
      "2025-12  30.465832  31.303543   32.168381  \n",
      "\n",
      "[12 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare for future predictions\n",
    "# Extract unique house numbers (guri_num) from your dataset\n",
    "valid_guri_nums = monthly_df['guri_num'].unique()\n",
    "\n",
    "# Generate future months for prediction\n",
    "future_dates = pd.date_range(start='2025-01-01', end='2027-12-31', freq='M').to_period('M')\n",
    "\n",
    "# Create a DataFrame for future predictions\n",
    "future_df = pd.DataFrame(index=future_dates, columns=valid_guri_nums).fillna(0)\n",
    "\n",
    "# Combine historical and future data\n",
    "combined_df = pd.concat([pivot_df, future_df])\n",
    "\n",
    "# Prepare data for future prediction\n",
    "X_future, _ = create_sequences(combined_df.values, SEQ_LENGTH)\n",
    "\n",
    "# Make future predictions\n",
    "future_predictions = model.predict(X_future[-len(future_dates):])\n",
    "\n",
    "# Inverse transform the future predictions\n",
    "future_predictions_inverse = scaler.inverse_transform(future_predictions.reshape(-1, 1)).reshape(future_predictions.shape)\n",
    "\n",
    "# Create a DataFrame for future predictions\n",
    "future_predictions_df = pd.DataFrame(future_predictions_inverse, index=future_dates, columns=valid_guri_nums)\n",
    "\n",
    "# Save the future predictions to CSV\n",
    "# future_predictions_df.to_csv('future_predictions.csv')\n",
    "\n",
    "print(future_predictions_df.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd821011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         BOO13096   BOO14883   BOO16425   BOO21552   BOO24958   BOO25971  \\\n",
      "2026-01  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-02  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-03  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-04  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-05  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-06  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-07  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-08  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-09  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-10  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-11  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2026-12  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-01  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-02  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-03  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-04  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-05  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-06  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-07  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-08  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-09  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-10  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-11  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "2027-12  27.38097  26.573942  19.100641  27.311792  30.702206  24.089138   \n",
      "\n",
      "          BOO26776  BOO27637   BOO34137   BOO38048  ...   YAA39553   YAA44919  \\\n",
      "2026-01  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-02  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-03  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-04  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-05  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-06  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-07  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-08  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-09  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-10  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-11  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2026-12  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-01  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-02  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-03  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-04  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-05  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-06  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-07  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-08  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-09  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-10  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-11  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "2027-12  22.460796  28.47122  32.163464  28.258417  ...  24.893847  30.802134   \n",
      "\n",
      "          YAA62105   YAA63403   YAA70435   YAA78103   YAA82926   YAA96787  \\\n",
      "2026-01  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-02  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-03  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-04  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-05  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-06  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-07  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-08  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-09  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-10  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-11  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2026-12  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-01  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-02  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-03  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-04  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-05  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-06  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-07  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-08  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-09  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-10  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-11  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "2027-12  28.397104  20.237024  31.078344  24.084578  33.705059  24.278681   \n",
      "\n",
      "          YAA97424   YAA97684  \n",
      "2026-01  26.544439  27.098694  \n",
      "2026-02  26.544439  27.098694  \n",
      "2026-03  26.544439  27.098694  \n",
      "2026-04  26.544439  27.098694  \n",
      "2026-05  26.544439  27.098694  \n",
      "2026-06  26.544439  27.098694  \n",
      "2026-07  26.544439  27.098694  \n",
      "2026-08  26.544439  27.098694  \n",
      "2026-09  26.544439  27.098694  \n",
      "2026-10  26.544439  27.098694  \n",
      "2026-11  26.544439  27.098694  \n",
      "2026-12  26.544439  27.098694  \n",
      "2027-01  26.544439  27.098694  \n",
      "2027-02  26.544439  27.098694  \n",
      "2027-03  26.544439  27.098694  \n",
      "2027-04  26.544439  27.098694  \n",
      "2027-05  26.544439  27.098694  \n",
      "2027-06  26.544439  27.098694  \n",
      "2027-07  26.544439  27.098694  \n",
      "2027-08  26.544439  27.098694  \n",
      "2027-09  26.544439  27.098694  \n",
      "2027-10  26.544439  27.098694  \n",
      "2027-11  26.544439  27.098694  \n",
      "2027-12  26.544439  27.098694  \n",
      "\n",
      "[24 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "print(future_predictions_df.tail(24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3327406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a98efe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3679791091.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    import lightning L\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('modified_electricity_consumption_data2.csv')\n",
    "\n",
    "# Convert 'datetime' to datetime data type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Aggregate daily data to monthly data\n",
    "df['year_month'] = df['datetime'].dt.to_period('M')\n",
    "monthly_df = df.groupby(['year_month', 'guri_num'])['total_KW'].sum().reset_index()\n",
    "\n",
    "# Convert 'year_month' to datetime for proper ordering\n",
    "monthly_df['year_month'] = monthly_df['year_month'].dt.to_timestamp()\n",
    "\n",
    "# Normalize the target variable\n",
    "scaler = MinMaxScaler()\n",
    "monthly_df['total_KW'] = scaler.fit_transform(monthly_df[['total_KW']])\n",
    "\n",
    "# Create index column for TimeSeriesDataSet\n",
    "monthly_df['time_idx'] = (monthly_df['year_month'] - monthly_df['year_month'].min()).dt.days // 30\n",
    "\n",
    "# Prepare dataset\n",
    "max_encoder_length = 12  # 12 months of history\n",
    "max_prediction_length = 12  # 12 months to predict\n",
    "\n",
    "training_cutoff = monthly_df['time_idx'].max() - max_prediction_length\n",
    "training = TimeSeriesDataSet(\n",
    "    monthly_df[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"total_KW\",\n",
    "    group_ids=[\"guri_num\"],\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"guri_num\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\"total_KW\"],\n",
    "    target_normalizer=NaNLabelEncoder(add_nan=True),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, monthly_df, predict=True, stop_randomization=True)\n",
    "\n",
    "batch_size = 64  # set this according to your system's capabilities\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "# Define TFT model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,  # set to >0 to use GPU\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # for faster training, remove/comment for full training\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "# Make predictions\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Select the last max_prediction_length months\n",
    "predictions, index = best_tft.predict(val_dataloader, return_index=True)\n",
    "actuals = validation.df[validation.df.time_idx > training_cutoff]\n",
    "\n",
    "# Inverse transform predictions\n",
    "predicted_values = scaler.inverse_transform(predictions.numpy().reshape(-1, 1)).reshape(predictions.shape)\n",
    "\n",
    "# Create a DataFrame for actual and predicted values\n",
    "predicted_df = pd.DataFrame(predicted_values.flatten(), columns=['Predicted'], index=index)\n",
    "actual_df = actuals[['time_idx', 'guri_num', 'total_KW']].set_index(index)\n",
    "\n",
    "# Concatenate the actual and predicted DataFrames\n",
    "comparison_df = pd.concat([actual_df, predicted_df], axis=1)\n",
    "\n",
    "print(comparison_df.head(12))\n",
    "\n",
    "# Save the future predictions to CSV\n",
    "# comparison_df.to_csv('future_predictions_tft.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c203b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
